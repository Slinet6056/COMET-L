llm:
  base_url: "https://api.openai.com/v1"
  api_key: "your-api-key-here"
  model: "gpt-4"
  temperature: 0.7
  max_tokens: 4096
  supports_json_mode: true # 如果使用本地模型不支持 JSON 模式，请设置为 false
  timeout: 120  # API 请求超时时间（秒）
  reasoning_effort: null  # 推理努力程度，可能的可选值: 'none', 'minimal', 'low', 'medium', 'high'，null 表示使用默认值
  verbosity: null  # 响应详细程度，可能的可选值: 'low', 'medium', 'high'，null 表示使用默认值

execution:
  timeout: 300  # 秒
  test_timeout: 30  # 测试执行超时时间（秒），默认30秒快速失败
  coverage_timeout: 300  # 覆盖率收集超时时间（秒），JaCoCo需要更多时间
  max_retries: 3
  maven_home: null  # 留空使用系统默认

paths:
  workspace: "./workspace"
  cache: "./cache"
  output: "./output"
  sandbox: "./sandbox"

evolution:
  max_iterations: 10
  min_improvement_threshold: 0.01
  budget_llm_calls: 1000
  stop_on_no_improvement_rounds: 3
  # 优秀水平阈值（达到后可提前停止，可根据项目调整）
  excellent_mutation_score: 0.95      # 变异分数阈值（默认 95%）
  excellent_line_coverage: 0.90       # 行覆盖率阈值（默认 90%）
  excellent_branch_coverage: 0.85     # 分支覆盖率阈值（默认 85%）
  min_method_lines: 5                 # 目标方法的最小行数，小于此值的方法将被跳过

knowledge:
  enable_dynamic_update: true
  pattern_confidence_threshold: 0.5
  contract_extraction_enabled: true

logging:
  level: "INFO"
  file: "comet.log"

preprocessing:
  enabled: true  # 是否启用并行预处理
  max_workers: null  # 最大并发数，null表示自动（cpu_count）
  timeout_per_method: 300  # 单个方法的超时时间（秒）
